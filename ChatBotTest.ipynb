{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeBERTEmbedder:\n",
    "    def __init__(self):\n",
    "        # Pre-set model name to the standard CodeBERT model\n",
    "        model_name = \"microsoft/codebert-base\"\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        self.model = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "    def generate_embedding(self, text):\n",
    "        # Tokenize input text\n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        # Generate embeddings using CodeBERT (no gradient computation needed)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**tokens)\n",
    "        # Perform mean pooling to obtain a single vector representation\n",
    "        return output.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    def batch_generate_embeddings(self, texts):\n",
    "        # Generate embeddings for a batch of texts\n",
    "        embeddings = [self.generate_embedding(text) for text in texts]\n",
    "        # Concatenate embeddings into a single tensor\n",
    "        return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        self.dim = 768  # Pre-set embedding dimension for CodeBERT\n",
    "        self.index = faiss.IndexFlatIP(self.dim)  # Use inner product for similarity\n",
    "\n",
    "    def add_vectors(self, embeddings):\n",
    "        # Convert PyTorch tensors to NumPy for FAISS compatibility\n",
    "        if isinstance(embeddings, torch.Tensor):\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "    def search(self, query_embedding, top_k=5):\n",
    "        if isinstance(query_embedding, torch.Tensor):\n",
    "            query_embedding = query_embedding.cpu().numpy()\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    def __init__(self):\n",
    "        self.embedder = CodeBERTEmbedder()\n",
    "        self.vector_store = VectorStore()\n",
    "\n",
    "    def add_contexts(self, texts):\n",
    "        # Generate embeddings for the provided contexts and store them\n",
    "        embeddings = self.embedder.batch_generate_embeddings(texts)\n",
    "        self.vector_store.add_vectors(embeddings)\n",
    "\n",
    "    def retrieve_context(self, query, top_k=5):\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedder.generate_embedding(query)\n",
    "        distances, indices = self.vector_store.search(query_embedding, top_k)\n",
    "        return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPChatbot:\n",
    "    def __init__(self):\n",
    "        self.retriever = RAGRetriever()\n",
    "        self.system_message = \"I am here to assist with Competitive Programming problems.\"\n",
    "\n",
    "    def add_knowledge_base(self, contexts):\n",
    "        # Add contexts (editorials or metadata) to the vector store\n",
    "        self.retriever.add_contexts(contexts)\n",
    "\n",
    "    def chat(self, query):\n",
    "        # Retrieve relevant contexts (indices and distances)\n",
    "        distances, indices = self.retriever.retrieve_context(query, top_k=1)\n",
    "        \n",
    "        # Fetch the most relevant problem from the original dataset based on the first index\n",
    "        most_relevant_problem = data[indices[0][0]]\n",
    "\n",
    "        # Construct a response that includes all the details of the most relevant problem\n",
    "        response = \"Most relevant problem:\\n\"\n",
    "        response += f\"Title: {most_relevant_problem['title']}\\n\"\n",
    "        response += f\"Description: {most_relevant_problem['description']}\\n\"\n",
    "        response += f\"Time Limit: {most_relevant_problem['time_limit']}\\n\"\n",
    "        response += f\"Memory Limit: {most_relevant_problem['memory_limit']}\\n\"\n",
    "        response += f\"Tags: {', '.join(most_relevant_problem['tags'])}\\n\"\n",
    "        response += f\"Solution: {most_relevant_problem['solution']}\\n\"\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Problem-Data.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = CPChatbot()\n",
    "chatbot.add_knowledge_base(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant problem:\n",
      "Title: A. Catch the Coin\n",
      "Description: Monocarp visited a retro arcade club with arcade cabinets. There got curious about the \"Catch the Coin\" cabinet.\n",
      "The game is pretty simple. The screen represents a coordinate grid such that:\n",
      "the X-axis is directed from left to right;\n",
      "the Y-axis is directed from bottom to top;\n",
      "the center of the screen has coordinates $$$(0, 0)$$$.\n",
      "At the beginning of the game, the character is located in the center, and $$$n$$$ coins appear on the screen — the $$$i$$$-th coin is at coordinates $$$(x_i, y_i)$$$. The coordinates of all coins are different and not equal to $$$(0, 0)$$$.\n",
      "In one second, Monocarp can move the character in one of eight directions. If the character is at coordinates $$$(x, y)$$$, then it can end up at any of the coordinates $$$(x, y + 1)$$$, $$$(x + 1, y + 1)$$$, $$$(x + 1, y)$$$, $$$(x + 1, y - 1)$$$, $$$(x, y - 1)$$$, $$$(x - 1, y - 1)$$$, $$$(x - 1, y)$$$, $$$(x - 1, y + 1)$$$.\n",
      "If the character ends up at the coordinates with a coin, then Monocarp collects that coin.\n",
      "After Monocarp makes a move, all coins fall down by $$$1$$$, that is, they move from $$$(x, y)$$$ to $$$(x, y - 1)$$$. You can assume that the game field is infinite in all directions.\n",
      "Monocarp wants to collect at least one coin, but cannot decide which coin to go for. Help him determine, for each coin, whether he can collect it.\n",
      "Input\n",
      "The first line contains a single integer $$$n$$$ ($$$1 \\le n \\le 500$$$) — the number of coins.\n",
      "In the $$$i$$$-th of the next $$$n$$$ lines, two integers $$$x_i$$$ and $$$y_i$$$ ($$$-50 \\le x_i, y_i \\le 50$$$) are written — the coordinates of the $$$i$$$-th coin. The coordinates of all coins are different. No coin is located at $$$(0, 0)$$$.\n",
      "Output\n",
      "For each coin, print \"YES\" if Monocarp can collect it. Otherwise, print \"NO\".\n",
      "Example\n",
      "input\n",
      "Copy\n",
      "5\n",
      "24 42\n",
      "-2 -1\n",
      "-1 -2\n",
      "0 -50\n",
      "15 0\n",
      "output\n",
      "Copy\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "Note\n",
      "Pay attention to the second coin in the example. Monocarp can first move from $$$(0, 0)$$$ to $$$(-1, -1)$$$. Then the coin falls $$$1$$$ down and ends up at $$$(-2, -2)$$$. Finally, Monocarp moves to $$$(-2, -2)$$$ and collects the coin.\n",
      "Time Limit: 2 seconds\n",
      "Memory Limit: 256 megabytes\n",
      "Tags: implementation, *800\n",
      "Solution: / /&&print$'>-2?Yes:No,$/for<>#FREE_KURSK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you tell me about Catch the coin problem?\"\n",
    "response = chatbot.chat(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
